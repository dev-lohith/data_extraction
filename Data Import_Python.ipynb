{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5ece47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ways to Import data\n",
    "• Flat files, e.g. .txts, .csvs\n",
    "• Files from other software\n",
    "• Relational databases\n",
    "\n",
    "# Reading a text file\n",
    "filename = 'huck_finn.txt'\n",
    "file = open(filename, mode='r') # 'r' is to read\n",
    "text = file.read()\n",
    "#read one line - file.readline()\n",
    "file.close()\n",
    "    # print(text) - prints the text\n",
    "    # Check whether file is closed\n",
    "    print(file.closed)\n",
    "    \n",
    "# Context manager with\n",
    "with open('huck_finn.txt', 'r') as file:\n",
    "    print(file.read())   # prints the text\n",
    "    \n",
    "# Why NumPy?\n",
    "NumPy arrays: standard for storing numerical data\n",
    "Essential for other packages: e.g. scikit-learn\n",
    "loadtxt()\n",
    "genfromtxt()\n",
    "\n",
    "# importing flat files using numpy\n",
    "import numpy as np\n",
    "filename = 'MNIST.txt'\n",
    "data = np.loadtxt(filename, delimiter=',', skiprows=1, usecols=[0,2], dtype=str) #skips firstrow if header and takes only first and third column and takes the data as string\n",
    "data\n",
    "\n",
    "# importing flat files using pandas\n",
    "What a data scientist needs\n",
    "• Two-dimensional labeled data structure(s)\n",
    "• Columns of potentially different types\n",
    "• Manipulate, slice, reshape, groupby, join, merge\n",
    "• Perform statistics\n",
    "• Work with time series data\n",
    "\n",
    "# Manipulating pandas DataFrames\n",
    "• Exploratory data analysis\n",
    "• Data wrangling\n",
    "• Data preprocessing \n",
    "• Building models\n",
    "• Visualization\n",
    "\n",
    "import pandas as pd\n",
    "filename = 'winequality-red.csv'\n",
    "data = pd.read_csv(filename)\n",
    "data.head()\n",
    "\n",
    "# convert dataframe to numpy array\n",
    "data_array = data.values\n",
    "\n",
    "Other file types\n",
    "• Excel spreadsheets\n",
    "• MATLAB files\n",
    "• SAS files\n",
    "• Stata files\n",
    "• HDF5 files\n",
    "\n",
    "Pickled files\n",
    "• File type native to Python\n",
    "• Motivation: many datatypes for which it isn't obvious how to store them\n",
    "• Pickled files are serialized\n",
    "• Serialize = convert object to bytestream\n",
    "\n",
    "# Pickled files\n",
    "import pickle\n",
    "with open('pickled_fruit.pkl', 'rb') as file:  #rb refers to read-only and binary\n",
    "    data = pickle.load(file)\n",
    "print(data)\n",
    "\n",
    "# Importing Excel sheets\n",
    "import pandas as pd\n",
    "file = 'urbanpop.xlsx'\n",
    "data = pd.ExcelFile(file)\n",
    "print(data.sheet_names) #ex - ['1960-1966', '1967-1974', '1975-2011']\n",
    "\n",
    "df1 = data.parse('1960-1966') # sheet name as a string\n",
    "df2 = data.parse(0) # sheet index as a float\n",
    "\n",
    "# Importing SAS/Stata files using pandas\n",
    "SAS and Stata files\n",
    "• SAS: Statistical Analysis System\n",
    "• Stata: \"Statistics\" + \"data\"\n",
    "• SAS: business analytics and biostatistics\n",
    "• Stata: academic social sciences research\n",
    "    \n",
    "SAS files\n",
    "• Used for:\n",
    "    • Advanced analytics\n",
    "    • Multivariate analysis\n",
    "    • Business intelligence\n",
    "    • Data management\n",
    "    • Predictive analytics\n",
    "    • Standard for computational analysis\n",
    "    \n",
    "# Importing SAS files\n",
    "import pandas as pd\n",
    "from sas7bdat import SAS7BDAT\n",
    "\n",
    "with SAS7BDAT('urbanpop.sas7bdat') as file:\n",
    "    df_sas = file.to_data_frame()\n",
    "    \n",
    "#Importing Stata files\n",
    "import pandas as pd\n",
    "data = pd.read_stata('urbanpop.dta')\n",
    "\n",
    "HDF5 files\n",
    "• Hierarchical Data Format version 5\n",
    "• Standard for storing large quantities of numerical data\n",
    "• Datasets can be hundreds of gigabytes or terabytes\n",
    "• HDF5 can scale to exabytes\n",
    "\n",
    "#Importing HDF5 files\n",
    "import h5py\n",
    "filename = 'H-H1_LOSC_4_V1-815411200-4096.hdf5'\n",
    "data = h5py.File(filename, 'r')  # \"r\" is to read\n",
    "print(type(data))\n",
    "\n",
    "The structure of HDF5 files\n",
    "for key in data.keys():\n",
    "    print(key)  # prints meta \\n quality \\n strain\n",
    "    \n",
    "print(type(data(['meta']))) # prints <class 'h5py._h1.group.Group'>\n",
    "\n",
    "The structure of HDF5 files\n",
    "for key in data['meta'].keys():\n",
    "    print(key)\n",
    "    \n",
    "# prints the following\n",
    "    Description DescriptionURL\n",
    "    • Detector\n",
    "    • Duration\n",
    "    • GPSstart\n",
    "    • Observatory\n",
    "    • Type\n",
    "    • UTCstart\n",
    "\n",
    "print(np.array(data['meta']['Description']), np.array(data['meta']['Detector']))\n",
    "#prints  b'Strain data time series from LIGO' b'H1'\n",
    "\n",
    "# MATLAB\n",
    "• \"Matrix Laboratory\"\n",
    "• Industry standard in engineering and science\n",
    "• Data saved as .mat files\n",
    "\n",
    "SciPy to the rescue!\n",
    "• scipy.io.loadmat()- read .mat files\n",
    "• scipy.io.savemat() - write .mat files\n",
    "\n",
    "#Importing a .mat file\n",
    "import scipy.io\n",
    "filename = 'workspace.mat'\n",
    "mat = scipy.io.loadmat(filename)\n",
    "print(type(mat))    # prints <class 'dict'>\n",
    "\n",
    "keys = MATLAB variable names\n",
    "values = objects assigned to variables\n",
    "\n",
    "print(type(mat['x'])) # <class 'numpy.ndarray'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26506f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relational databases\n",
    "What is a relational database?\n",
    "• Based on relational model of data\n",
    "• First described by Edgar “Ted” Codd\n",
    "\n",
    "Relational model\n",
    "• Widely adopted\n",
    "• Codd's 12 Rules/Commandments\n",
    "    • Consists of 13 rules (zero-indexed!)\n",
    "    • Describes what a Relational Database Management System should adhere to to be considered relational\n",
    "    \n",
    "Relational Database Management Systems\n",
    "• PostgreSQL\n",
    "• MySQL\n",
    "• SQLite\n",
    "• SQL=Structured Query Language\n",
    "\n",
    "#Creating a database engine\n",
    "• SQLite database\n",
    "• Fast and simple\n",
    "• SQLAlchemy\n",
    "• Works with many Relational Database Management Systems\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite:///Northwind.sqlite')\n",
    "\n",
    "table_names = engine.table_names()\n",
    "print(table_names)\n",
    "#prints\n",
    "    #['Categories', 'Customers', 'EmployeeTerritories', 'Employees', 'Order Details', 'Orders', 'Products', 'Region', 'Shippers', 'Suppliers', 'Territories']\n",
    "    \n",
    "Workflow of SQL querying\n",
    "• Import packages and functions\n",
    "• Create the database engine\n",
    "• Connect to the engine\n",
    "• Query the database\n",
    "• Save query results to a DataFrame\n",
    "• Close the connection\n",
    "\n",
    "# SQL query\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "engine = create_engine('sqlite:///Northwind.sqlite')\n",
    "con = engine.connect()\n",
    "rs = con.execute('SELECT * FROM Orders')\n",
    "df = pd.DataFrame(rs.fetchall())\n",
    "con.close()\n",
    "\n",
    "# Using the context managers\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "engine = create_engine('sqlite:///Northwind.sqlite')\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute(\"SELECT OrderID, OrderDate, ShipName FROM Orders\")\n",
    "    df = pd.DataFrame(rs.fetchmany(size=5))  # size=5 loads only 5 rows\n",
    "    df.columns = rs.keys()\n",
    "\n",
    "# example\n",
    "    # Create engine: engine\n",
    "    engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "    # Open engine in context manager\n",
    "    # Perform query and save results to DataFrame: df\n",
    "    with engine.connect() as con:\n",
    "        rs = con.execute('SELECT * FROM Employee WHERE EmployeeId >= 6')\n",
    "        df = pd.DataFrame(rs.fetchall())\n",
    "        df.columns = rs.keys()\n",
    "\n",
    "    # Print the head of the DataFrame df\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0654d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quering relatiional databases directly with pandas\n",
    "# Pandas way to query\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "engine = create_engine('sqlite:///Northwind.sqlite')\n",
    "\n",
    "df = pd.read_sql_query(\"SELECT * FROM Orders\", engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b52d813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediate importing data in python\n",
    "• Flat files such as .txt and .csv\n",
    "• Pickled files, Excel spreadsheets, and many others!\n",
    "• Data from relational databases\n",
    "• You can do all these locally\n",
    "• What if your data is online?\n",
    "\n",
    "• Import and locally save datasets from the web\n",
    "• Load datasets into pandas DataFrames\n",
    "• Make HTTP requests (GET requests)\n",
    "• Scrape web data such as HTML\n",
    "• Parse HTML into useful data (BeautifulSoup)\n",
    "• Use the urllib and requests packages\n",
    "\n",
    "The urllib package\n",
    "• Provides interface for fetching data across the web\n",
    "• urlopen() - accepts URLs instead of file names\n",
    "\n",
    "# Automate file download in python\n",
    "from urllib.request import urlretrieve\n",
    "url = \"http://archive.ics.ucl.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\"\n",
    "urlretrieve(url, \"winequallity-white.csv\") # retrieves files from url and save as winequallity-white.csv\n",
    "\n",
    "URL\n",
    "• Uniform/Universal Resource Locator\n",
    "• References to web resources\n",
    "• Focus: web addresses\n",
    "Ingredients:\n",
    "• Protocol identifier - http:\n",
    "。 Resource name - datacamp.com\n",
    "• These specify web addresses uniquely\n",
    "\n",
    "HTTP\n",
    "• HyperText Transfer Protocol\n",
    "• Foundation of data communication for the web\n",
    "• HTTPS - more secure form of HTTP\n",
    "Going to a website = sending HTTP request 。 GET request\n",
    "• urlretrieve () performs a GET request\n",
    "• HTML - HyperText Markup Language\n",
    "\n",
    "# Get requests from urllib\n",
    "from urllib.request import urlopen, Requests\n",
    "url = \"http://www.wikipedia.org/\"\n",
    "request = Request(url)\n",
    "response = urlopen(request)\n",
    "html = response.read()\n",
    "response.close()\n",
    "\n",
    "# Get requests using requests\n",
    "import requests\n",
    "url = \"https://www.wikipedia.org\"\n",
    "r = requests.get(url)\n",
    "test = r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101d0968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping the web using the python\n",
    "HTML\n",
    "• Mix of unstructured and structured data\n",
    "• Structured data:\n",
    "    • Has pre-defined data model, or\n",
    "    • Organized in a defined manner\n",
    "• Unstructured data: neither of these properties\n",
    "    \n",
    "BeautifulSoup\n",
    "• Parse and extract structured data from HTML\n",
    "• Make tag soup beautiful and extract information\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "url = 'https://www.crummy.com/software/BeautifulSoup/'\n",
    "request = requests.get(url)\n",
    "html_doc = r.text\n",
    "soup = BeautifulSoup(html_doc)\n",
    "\n",
    "print(soup.prettify())\n",
    "print(soup.title)\n",
    "print(soup.get_text())\n",
    "\n",
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c02dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# APIs\n",
    "• Application Programming Interface\n",
    "• Set of protocols and routines\n",
    "• Bunch of code\n",
    "• Allows two software programs to communicate with each other\n",
    "• Building and interacting with software applications\n",
    "\n",
    "# JSONS\n",
    "• JavaScript Object Notation\n",
    "• Real-time server-to-browser communication\n",
    "• Douglas Crockford\n",
    "• Human readable\n",
    "\n",
    "# Loading JSON in python\n",
    "import json \n",
    "with open('snakes.json', 'r') as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "    \n",
    "type(json_data) # prints dict\n",
    "\n",
    "#Exploring JSONs in python\n",
    "for key, value in json_data.items():\n",
    "    print(key + ':', value)\n",
    "    \n",
    "# Connectiing to an API in python\n",
    "import requests\n",
    "url = 'http://www.omdbapi.com/?t=hackers'\n",
    "r = requests.get(url)\n",
    "json_data = r.json()\n",
    "for key, value in json_data.items():\n",
    "    print(key + ':', value)\n",
    "    \n",
    "What was that URL?\n",
    "• http - making an HTTP request\n",
    "• www.omdbapi.com - querying the OMDB API\n",
    "• ?t-hackers\n",
    "    • Query string\n",
    "    • Return data for a movie with title (t) 'Hackers' 'http://www.omdbapi.com/?t=hackers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8974113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Twitter API and authentication\n",
    "Herein, you'll learn\n",
    "• How to stream data from the Twitter API\n",
    "• How to filter incoming tweets for keywords\n",
    "• About API Authentication and OAuth\n",
    "• How to use the Tweepy Python package\n",
    "\n",
    "# Using Tweepy: Authenticatioon\n",
    "import tweepy, json\n",
    "access_token = \"...\"\n",
    "access_token_secret = \"...\"\n",
    "consumer_key = \"...\"\n",
    "consumer_secret = \"...\"\n",
    "\n",
    "# Create streaming object\n",
    "stream = tweepy.Stream(consumer_key, consumer_secret, access_token, access_token_secret)\n",
    "\n",
    "# This line filters twitter streams to capture data by keywords\n",
    "stream.filter(track = ['apples', 'oranges'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
